{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy\n",
    "import random\n",
    "import pandas\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "class QLearn:\n",
    "    def __init__(self, actions, epsilon, alpha, gamma,epsilon_decay):\n",
    "        self.q = {}\n",
    "        self.epsilon = epsilon  # exploration constant\n",
    "        self.alpha = alpha      # discount constant\n",
    "        self.epsilon_decay = epsilon_decay     # Epsilon Decay constant\n",
    "        self.gamma = gamma      # discount factor\n",
    "        self.actions = actions\n",
    "\n",
    "    def getQ(self, state, action):\n",
    "        return self.q.get((state, action), 0.0)\n",
    "\n",
    "    def learnQ(self, state, action, reward, value):\n",
    "        '''\n",
    "        Q-learning:\n",
    "            Q(s, a) += alpha * (reward(s,a) + max(Q(s') - Q(s,a))\n",
    "        '''\n",
    "        oldq = self.q.get((state, action), None)\n",
    "        if oldq is None:\n",
    "            self.q[(state, action)] = reward\n",
    "        else:\n",
    "            self.q[(state, action)] = oldq + self.alpha * (value - oldq)\n",
    "            \n",
    "\n",
    "    def chooseAction(self, state, return_q=False):\n",
    "        q = [self.getQ(state, a) for a in self.actions]\n",
    "        maxQ = max(q)\n",
    "\n",
    "        if random.random() < self.epsilon:\n",
    "            self.epsilon = self.epsilon*self.epsilon_decay\n",
    "            minQ = min(q); mag = max(abs(minQ), abs(maxQ))\n",
    "            # add random values to all the actions, recalculate maxQ\n",
    "            q = [q[i] + random.random() * mag - .5 * mag for i in range(len(self.actions))]\n",
    "            maxQ = max(q)\n",
    "\n",
    "        count = q.count(maxQ)\n",
    "        # In case there're several state-action max values\n",
    "        # we select a random one among them\n",
    "        if count > 1:\n",
    "            best = [i for i in range(len(self.actions)) if q[i] == maxQ]\n",
    "            i = random.choice(best)\n",
    "        else:\n",
    "            i = q.index(maxQ)\n",
    "\n",
    "        action = self.actions[i]\n",
    "        if return_q: # if they want it, give it!\n",
    "            return action, q\n",
    "        return action\n",
    "\n",
    "    def learn(self, state1, action1, reward, state2):\n",
    "        maxqnew = max([self.getQ(state2, a) for a in self.actions])\n",
    "        #maxqnew = self.getQ(state2, action) SARSA\n",
    "        \n",
    "        self.learnQ(state1, action1, reward, reward + self.gamma*maxqnew)\n",
    "\n",
    "def build_state(features):\n",
    "    return int(\"\".join(map(lambda feature: str(int(feature)), features)))\n",
    "\n",
    "def to_bin(value, bins):\n",
    "    return numpy.digitize(x=[value], bins=bins)[0]\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('CartPole-v0')\n",
    "    \n",
    "    env = gym.wrappers.Monitor(env, r'C:\\Users\\zarat\\OneDrive\\Msc Data Science\\Software Agents\\Logs', force=True)\n",
    "\n",
    "    goal_average_steps = 195\n",
    "    max_number_of_steps = 200\n",
    "    last_time_steps = numpy.ndarray(0)\n",
    "    n_bins = 8\n",
    "    n_bins_angle = 10\n",
    "\n",
    "    number_of_features = env.observation_space.shape[0]\n",
    "    last_time_steps = numpy.ndarray(0)\n",
    "    Rewards = numpy.ndarray(0)\n",
    "\n",
    "    # Number of states is huge so in order to simplify the situation\n",
    "    # we discretize the space to: 10 ** 4\n",
    "    cart_position_bins = pandas.cut([-2.4, 2.4], bins=n_bins, retbins=True)[1][1:-1]\n",
    "    pole_angle_bins = pandas.cut([-2, 2], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "    cart_velocity_bins = pandas.cut([-1, 1], bins=n_bins, retbins=True)[1][1:-1]\n",
    "    angle_rate_bins = pandas.cut([-3.5, 3.5], bins=n_bins_angle, retbins=True)[1][1:-1]\n",
    "\n",
    "\n",
    "    # The Q-learning algorithm\n",
    "    qlearn = QLearn(actions=range(env.action_space.n),\n",
    "                    alpha=0.5, gamma=0.90, epsilon=1, epsilon_decay = 0.999)\n",
    "\n",
    "    for i_episode in range(10000):\n",
    "        observation = env.reset()\n",
    "\n",
    "        cart_position, pole_angle, cart_velocity, angle_rate_of_change = observation            \n",
    "        state = build_state([to_bin(cart_position, cart_position_bins),\n",
    "                         to_bin(pole_angle, pole_angle_bins),\n",
    "                         to_bin(cart_velocity, cart_velocity_bins),\n",
    "                         to_bin(angle_rate_of_change, angle_rate_bins)])\n",
    "        \n",
    "        cumulated_reward = 0\n",
    "\n",
    "        for t in range(max_number_of_steps):\t    \t\n",
    "            # env.render()\n",
    "\n",
    "            # Pick an action based on the current state\n",
    "            action = qlearn.chooseAction(state)\n",
    "            # Execute the action and get feedback\n",
    "            observation, reward, done, info = env.step(action)\n",
    "\n",
    "            # Digitize the observation to get a state\n",
    "            cart_position, pole_angle, cart_velocity, angle_rate_of_change = observation            \n",
    "            nextState = build_state([to_bin(cart_position, cart_position_bins),\n",
    "                             to_bin(pole_angle, pole_angle_bins),\n",
    "                             to_bin(cart_velocity, cart_velocity_bins),\n",
    "                             to_bin(angle_rate_of_change, angle_rate_bins)])\n",
    "\n",
    "            if not(done):\n",
    "                qlearn.learn(state, action, reward, nextState)\n",
    "                state = nextState\n",
    "                cumulated_reward += reward \n",
    "                \n",
    "            else:\n",
    "                # Q-learn \n",
    "                reward = -500\n",
    "                qlearn.learn(state, action, reward, nextState)\n",
    "                last_time_steps = numpy.append(last_time_steps, [int(t + 1)])\n",
    "                cumulated_reward += reward\n",
    "                break\n",
    "\n",
    "        l = last_time_steps.tolist()\n",
    "        l.sort()\n",
    "        print(\"Overall score: {:0.2f}\".format(last_time_steps.mean()))\n",
    "        \n",
    "        Rewards = numpy.append(Rewards, [int(cumulated_reward)])\n",
    "        print(\"Episode {:d} reward score: {:0.2f}\".format(i_episode, cumulated_reward))\n",
    "\n",
    "    env.close()\n",
    "    \n",
    "Rewards = pandas.DataFrame(Rewards)\n",
    "l = pandas.DataFrame(l)\n",
    "Rewards.to_csv(\"default_Rewards.csv\")\n",
    "l.to_csv(\"default_Steps.csv\")\n",
    "\n",
    "import os\n",
    "os._exit(00)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
